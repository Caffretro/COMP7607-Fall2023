{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd-mtF0PTrv"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c9YBduQCI4"
      },
      "source": [
        "## 1 $n$-gram Language Model\n",
        "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) {=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ_Z1g8QZef"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "$ p(\\vec{w})$ =  $ p(w_1) ⋅ p(w_2 \\mid w_1) ⋅ p(w_3 \\mid w_1, w_2) ⋅(w_4 \\mid w_2, w_3) ... p(w_n \\mid w_{n-2}, w_{n-1})$ \\\\\n",
        "$ = \\dfrac{C(w_1)}{C(*)} \\dfrac{C(w_1~w_2)}{C(w_1)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgExbf1QtCH"
      },
      "source": [
        "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
        "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
        "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1ZGFIaRPCP"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "The MLE of $ p(\\vec{w})$ is $ C(w_1) \\dfrac{C(w_1~w_2)}{C(w_1)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$ \\\\\n",
        "\n",
        "Which can be canceled to: \\\\\n",
        "\n",
        "$ C(w_1~w_2~w_3) \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$ \\\\\n",
        "\n",
        "Similarly, we can write the MLE of $ p_{reversed}(\\vec{w}) $ to: $ C(w_n) \\dfrac{C(w_{n-1}~w_n)}{C(w_n)} \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-1}~w_n)} \\dfrac{C(w_{n-3}~w_{n-2}~w_{n-1})}{C(w_{n-2}~w_{n-1})} ... \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} $ \\\\\n",
        "\n",
        "Which can also be canceled to: \\\\\n",
        "\n",
        "$ {C(w_{n-2} ~w_{n-1} ~w_n)} \\dfrac{C(w_{n-3}~w_{n-2}~w_{n-1})}{C(w_{n-2}~w_{n-1})} ... \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} $ \\\\\n",
        "\n",
        "This is equavalent to the answer above, if we multiply all the denominators and numerators respectively, the composition are equavalent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEc5kz4RniG"
      },
      "source": [
        "## 2 $N$-gram Language Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "3kSwtN79jWgp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-22 14:34:56--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
            "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度：6640478 (6.3M) [text/plain]\n",
            "正在保存至: “train.txt”\n",
            "\n",
            "train.txt           100%[===================>]   6.33M  7.26MB/s  用时 0.9s      \n",
            "\n",
            "2023-10-22 14:34:57 (7.26 MB/s) - 已保存 “train.txt” [6640478/6640478])\n",
            "\n",
            "--2023-10-22 14:34:57--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
            "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度：872910 (852K) [text/plain]\n",
            "正在保存至: “dev.txt”\n",
            "\n",
            "dev.txt             100%[===================>] 852.45K  2.44MB/s  用时 0.3s      \n",
            "\n",
            "2023-10-22 14:34:58 (2.44 MB/s) - 已保存 “dev.txt” [872910/872910])\n",
            "\n",
            "--2023-10-22 14:34:58--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt\n",
            "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度：869318 (849K) [text/plain]\n",
            "正在保存至: “test.txt”\n",
            "\n",
            "test.txt            100%[===================>] 848.94K  2.48MB/s  用时 0.3s      \n",
            "\n",
            "2023-10-22 14:34:59 (2.48 MB/s) - 已保存 “test.txt” [869318/869318])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9HCVQwqkTc_"
      },
      "source": [
        "### 2.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhFKCzwkaTn"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<s>', 'facebook', 'has', 'released', 'a', 'report', 'that', 'shows', 'what', 'content', 'was', 'most', 'widely', 'viewed', 'by', 'americans', 'between', 'april', 'and', 'june', '</s>'], ['<s>', 'it', 'contains', 'sections', 'showing', 'the', 'top', '20', 'domains', 'links', 'pages', 'and', 'posts', 'in', 'terms', 'of', 'views', '</s>'], ['<s>', 'a', 'companion', 'guide', 'that', 'describes', 'how', 'the', 'data', 'was', 'gathered', 'and', 'analyzed', 'was', 'also', 'released', '</s>'], ['<s>', 'facebook', 'released', 'the', 'data', 'in', 'response', 'to', 'reports', 'that', 'posts', 'from', 'rightwing', 'sources', 'had', 'the', 'most', 'interaction', '</s>'], ['<s>', 'the', 'top', 'posts', 'only', 'account', 'for', 'less', 'than', '0', '</s>'], ['<s>', '1', 'percent', 'of', 'the', 'content', 'viewed', 'by', 'us', 'users', 'and', 'the', 'data', 'only', 'accounts', 'for', 'public', 'posts', 'not', 'posts', 'made', 'in', 'private', 'groups', '</s>'], ['<s>', 'a', 'link', 'to', 'the', 'report', 'is', 'available', 'in', 'the', 'article', '</s>'], ['<s>', 'a', 'quantum', 'effect', 'called', '<UNK>', 'allows', 'a', 'collection', 'of', 'molecules', 'to', 'absorb', 'light', 'more', 'efficiently', 'than', 'if', 'each', 'molecule', 'were', 'acting', 'individually', '</s>'], ['<s>', 'researchers', 'have', 'used', 'this', 'phenomenon', 'to', 'create', 'a', 'proofofconcept', 'quantum', 'battery', '</s>'], ['<s>', 'the', 'larger', 'the', 'battery', 'was', 'the', 'faster', 'it', 'charged', 'demonstrating', '<UNK>', 'at', 'work', '</s>']]\n",
            "[['<s>', 'bush', 'has', 'admitted', 'a', '<UNK>', 'with', 'london', 's', 'pubs', 'ahead', 'of', 'his', 'arrival', 'in', 'the', 'city', 'this', 'week', 'for', 'a', 'state', 'visit', 'to', 'britain', '</s>'], ['<s>', 'the', 'death', 'toll', 'in', 'the', 'collapse', 'of', 'a', '<UNK>', 'on', 'the', 'queen', 'mary', 'ii', 'has', 'risen', 'to', 'according', 'to', 'a', 'new', 'toll', 'issued', 'sunday', 'by', 'a', 'spokesman', 'for', 'alstom', 'which', 'owns', 'the', 'shipyard', '</s>'], ['<s>', 'the', 'match', '<UNK>', 'from', 'sunday', 's', 'friendly', 'between', 'italy', 'and', 'romania', 'will', 'go', 'to', 'the', 'families', 'of', 'those', 'who', 'died', 'in', 'this', 'week', 's', 'bomb', 'attack', 'on', 'an', 'italian', 'police', 'base', 'in', 'iraq', '</s>'], ['<s>', 'south', 'korean', 'foreign', 'and', 'trade', 'minister', '<UNK>', '<UNK>', 'was', 'due', 'in', 'moscow', 'late', 'sunday', 'for', 'a', 'threeday', 'visit', 'to', 'discuss', 'a', 'broad', 'range', 'of', 'regional', 'and', 'bilateral', 'issues', 'a', 'south', 'korean', 'embassy', 'official', 'said', '</s>'], ['<s>', 'points', 'on', 'the', 'richter', 'scale', 'occurred', 'off', 'the', 'western', 'greek', 'island', 'of', '<UNK>', 'early', 'on', 'sunday', 'the', 'athens', 'observatory', 'said', '</s>'], ['<s>', 'a', 'french', 'woman', 'working', 'for', 'the', 'un', 's', 'refugee', 'agency', 'unhcr', 'was', 'shot', 'dead', 'by', 'unknown', 'gunmen', 'sunday', 'afternoon', 'in', 'the', 'southeast', 'afghan', 'town', 'of', '<UNK>', 'a', 'un', 'spokesman', 'said', '</s>'], ['<s>', 'at', 'least', 'people', 'were', 'killed', 'in', 'bomb', 'attacks', 'at', 'two', 'synagogues', 'in', 'the', 'historic', 'turkish', 'city', 'of', 'istanbul', 'on', 'saturday', 'the', 'city', 's', 'health', 'department', 'said', 'sunday', '</s>'], ['<s>', 'pope', 'john', 'paul', 'ii', 'criticised', 'israel', 'on', 'sunday', 'for', 'building', 'a', 'barrier', 'in', 'the', 'west', 'bank', 'saying', 'the', 'middle', 'east', 'does', 'not', 'need', 'walls', 'but', 'bridges', '</s>'], ['<s>', 'a', 'french', 'woman', 'working', 'for', 'the', 'un', 's', 'high', 'commissioner', 'for', 'refugees', 'lrb', 'unhcr', 'rrb', 'was', 'shot', 'dead', 'by', 'unknown', 'gunmen', 'sunday', 'afternoon', 'in', 'the', 'southeast', 'afghan', 'town', 'of', '<UNK>', 'a', 'un', 'spokesman', 'said', '</s>'], ['<s>', 'president', 'chandrika', 'kumaratunga', 'has', 'denied', 'reports', 'she', 'has', 'offered', 'to', 'share', 'the', 'vital', 'defense', 'portfolio', 'with', 'her', 'estranged', 'prime', 'minister', 'her', 'spokesman', '<UNK>', 'said', 'sunday', '</s>']]\n",
            "Train vocabulary size: 17683\n",
            "Dev vocabulary size: 4914\n",
            "Test vocabulary size: 4660\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "def preprocess_file_nltk(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.readlines()\n",
        "\n",
        "    # Tokenize and preprocess the sentences\n",
        "    sentences = []\n",
        "    unpaded_sentences = []\n",
        "    for line in content:\n",
        "        line = re.sub(f\"[{string.punctuation}]\", \"\", line)\n",
        "        tokens = nltk.word_tokenize(line.strip())\n",
        "        unpaded_sentences.append(tokens)\n",
        "        tokens = list(pad_both_ends(tokens, n=2))  # Pad the sentence with <s> and </s>\n",
        "        sentences.append(tokens)\n",
        "    # print(sentences[:10])\n",
        "    # Create a frequency dictionary using nltk's FreqDist\n",
        "    freq_dict = nltk.FreqDist(token for tokens in sentences for token in tokens)\n",
        "    # print(sorted(freq_dict))\n",
        "    # only keep tokens that appear at least 3 times in the file first \n",
        "    vocab = Vocabulary(freq_dict, unk_cutoff=3)\n",
        "    return sentences, unpaded_sentences, vocab\n",
        "\n",
        "train_file = './train.txt'\n",
        "dev_file = './dev.txt'\n",
        "test_file = './test.txt'\n",
        "\n",
        "train_sentences_noUNK, train_sentences_unpad_noUNK, train_vocab = preprocess_file_nltk(train_file)\n",
        "train_sentences = [[word if word in train_vocab else \"<UNK>\" for word in sentence] for sentence in train_sentences_noUNK]\n",
        "train_sentences_unpad = [[word if word in train_vocab else \"<UNK>\" for word in sentence] for sentence in train_sentences_unpad_noUNK]\n",
        "\n",
        "dev_sentences_noUNK, dev_sentences_unpad, dev_vocab = preprocess_file_nltk(dev_file)\n",
        "dev_sentences = [[word if word in train_vocab else \"<UNK>\" for word in sentence] for sentence in dev_sentences_noUNK]\n",
        "test_sentences_noUNK, test_sentences_unpad, test_vocab = preprocess_file_nltk(test_file)\n",
        "test_sentences = [[word if word in train_vocab else \"<UNK>\" for word in sentence] for sentence in test_sentences_noUNK]\n",
        "print(train_sentences[:10])\n",
        "print(dev_sentences[:10])\n",
        "\n",
        "print(f\"Train vocabulary size: {len(train_vocab)}\")\n",
        "print(f\"Dev vocabulary size: {len(dev_vocab)}\")\n",
        "print(f\"Test vocabulary size: {len(test_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQNsUA5kfZe"
      },
      "source": [
        "**Discussion**\n",
        "Given the vocabulary size above, the number of parameters in a n-gram model is simply vocab size to its nth times, i.e. |V| ^ n. Since the number of parameters of a ngram model refers to the number of possible n grams that can be generated from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzDNMVikkeX"
      },
      "source": [
        "### 2.2 $N$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxkcs2HykuR2"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=3 unk_label='<UNK>' and 17683 items>\n",
            "<Vocabulary with cutoff=3 unk_label='<UNK>' and 17683 items>\n",
            "Train Unigram Model Perplexity: 968.4033991354514\n",
            "Train Bigram Model Perplexity: 72.38090357599297\n",
            "Dev Unigram Model Perplexity: 967.7749548528787\n",
            "Dev Bigram Model Perplexity: inf\n"
          ]
        }
      ],
      "source": [
        "from nltk import everygrams, ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten\n",
        "\n",
        "def train_language_model(n, train_data, vocab):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # Train the language model\n",
        "    lm = MLE(n, vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    print(lm.vocab)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "def compute_perplexity(n, lm, test_data):\n",
        "    # Preprocess the test data\n",
        "    test_ngrams = [ngram for sent in test_data for ngram in ngrams(sent, n)]\n",
        "    # print(test_ngrams[:10])\n",
        "    return lm.perplexity(test_ngrams)\n",
        "\n",
        "# Train models and calculate perplexity\n",
        "unigram_lm = train_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_language_model(2, train_sentences, train_vocab)\n",
        "\n",
        "train_unigram_perplexity = compute_perplexity(1, unigram_lm, train_sentences)\n",
        "train_bigram_perplexity = compute_perplexity(2, bigram_lm, train_sentences)\n",
        "\n",
        "dev_unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "dev_bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "# test_unigram_perplexity = compute_perplexity(1, unigram_lm, test_sentences)\n",
        "# test_bigram_perplexity = compute_perplexity(2, bigram_lm, test_sentences)\n",
        "\n",
        "print(\"Train Unigram Model Perplexity:\", train_unigram_perplexity)\n",
        "print(\"Train Bigram Model Perplexity:\", train_bigram_perplexity)\n",
        "\n",
        "print(\"Dev Unigram Model Perplexity:\", dev_unigram_perplexity)\n",
        "print(\"Dev Bigram Model Perplexity:\", dev_bigram_perplexity)\n",
        "\n",
        "# print(\"Test Unigram Model Perplexity:\", test_unigram_perplexity)\n",
        "# print(\"Test Bigram Model Perplexity:\", test_bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3o9Nez8kvYm"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "For training sets, since the language model is trained based on training data, a lower bigram perplexity is expected as bigram captures context information that unigram does not.\n",
        "However, the infinity from Bigram model comes from a 0 numerator, and it explodes the perplexity since the formula of perplexity inserts probability of guesses to a division's denominator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQUqM73kzf-"
      },
      "source": [
        "### 2.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LgXRmJwk3Y-"
      },
      "source": [
        "#### 2.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG7jCIRk7Qw"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Unigram Perplexity: 969.230602654412\n",
            "Train Bigram Perplexity: 1402.4365114712714\n",
            "Dev Unigram Perplexity: 969.1493297099746\n",
            "Dev Bigram Perplexity: 1639.0737296018713\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm import Laplace\n",
        "\n",
        "def train_laplace_language_model(n, train_data, vocab):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # Train the language model with Laplace smoothing\n",
        "    lm = Laplace(n, vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "\n",
        "    return lm\n",
        "\n",
        "# Train the models with Laplace smoothing\n",
        "unigram_lm = train_laplace_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_laplace_language_model(2, train_sentences, train_vocab)\n",
        "\n",
        "train_unigram_perplexity = compute_perplexity(1, unigram_lm, train_sentences)\n",
        "train_bigram_perplexity = compute_perplexity(2, bigram_lm, train_sentences)\n",
        "\n",
        "dev_unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "dev_bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "# test_unigram_perplexity = compute_perplexity(1, unigram_lm, test_sentences)\n",
        "# test_bigram_perplexity = compute_perplexity(2, bigram_lm, test_sentences)\n",
        "\n",
        "print(\"Train Unigram Perplexity:\", train_unigram_perplexity)\n",
        "print(\"Train Bigram Perplexity:\", train_bigram_perplexity)\n",
        "\n",
        "print(\"Dev Unigram Perplexity:\", dev_unigram_perplexity)\n",
        "print(\"Dev Bigram Perplexity:\", dev_bigram_perplexity)\n",
        "\n",
        "# print(\"Test Unigram Perplexity:\", test_unigram_perplexity)\n",
        "# print(\"Test Bigram Perplexity:\", test_bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yTKPXFk8f2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "After adding 1 to numerator and V to demoninator for each guess, the issue of exploding perplexity is controled through the introduction of non-zero terms in a division\n",
        "Notice that bigram perplexity has increased a lot for training data. This is because for each guess P(w_i | w_i-1, w_i-2), the behavior was already good on training data (pretty straightforward, since we are fitting the model on it). However, the added 1 and |V| to numerator and denominator in perplexity's conponents were flipped, and the size of V is overwhelming, even if we take a squareroot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cFbczqlBR_"
      },
      "source": [
        "#### 2.3.2: Add-$k$ smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_ZiAgIlPUu"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Unigram Perplexity: 968.4034233091518\n",
            "Train Bigram Perplexity: 113.10241373999663\n",
            "Dev Unigram Perplexity: 967.7781169242493\n",
            "Dev Bigram Perplexity: 378.2551205514041\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm import Lidstone\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def train_lidstone_language_model(n, train_data, vocab, lmda):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # Train the language model with Lidstone smoothing\n",
        "    lm = Lidstone(lmda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "# Train the models with Lidstone smoothing, which is just k smoothing.\n",
        "lmda = 5e-3\n",
        "\n",
        "unigram_lm = train_lidstone_language_model(1, train_sentences, train_vocab, lmda)\n",
        "bigram_lm = train_lidstone_language_model(2, train_sentences, train_vocab, lmda)\n",
        "\n",
        "train_unigram_perplexity = compute_perplexity(1, unigram_lm, train_sentences)\n",
        "train_bigram_perplexity = compute_perplexity(2, bigram_lm, train_sentences)\n",
        "\n",
        "dev_unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "dev_bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "# test_unigram_perplexity = compute_perplexity(1, unigram_lm, test_sentences)\n",
        "# test_bigram_perplexity = compute_perplexity(2, bigram_lm, test_sentences)\n",
        "\n",
        "print(\"Train Unigram Perplexity:\", train_unigram_perplexity)\n",
        "print(\"Train Bigram Perplexity:\", train_bigram_perplexity)\n",
        "\n",
        "print(\"Dev Unigram Perplexity:\", dev_unigram_perplexity)\n",
        "print(\"Dev Bigram Perplexity:\", dev_bigram_perplexity)\n",
        "\n",
        "# print(\"Test Unigram Perplexity:\", test_unigram_perplexity)\n",
        "# print(\"Test Bigram Perplexity:\", test_bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFNf8OIlQ0O"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "We can see that the perplexity of bigram is well controlled. Since add K smoothing is basically add 1 smoothing with coefficients k that we can define, we can use a relatively small k to control the unbalanced scale of 1 and V introduced by add 1 smoothing while also preventing zero division.\n",
        "I have tried the following K values: 0.01, 5e-3, 1e-3, 1e-4, 5e-4, 1e-5. and 5e-3 provided a relatively good performance among all parameters, and will be used for further model development on the same training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKEO_TqlUrX"
      },
      "source": [
        "#### 2.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdd4cvYlZuO"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training language models\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def train_mle_language_model(n, train_data, vocab, lidstone_lambda=5e-3):\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # lm = MLE(n, vocabulary=vocab)\n",
        "    lm = Lidstone(lidstone_lambda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    return lm\n",
        "\n",
        "unigram_lm = train_mle_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_mle_language_model(2, train_sentences, train_vocab)\n",
        "trigram_lm = train_mle_language_model(3, train_sentences, train_vocab)\n",
        "print(\"Finished training language models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k2/7hr6hcfd5v58p9nhhf_brbbm0000gn/T/ipykernel_16237/1286793746.py:17: RuntimeWarning: invalid value encountered in log\n",
            "  perplexity += np.log(sentence_perplexity)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best coefficients: [0.2, 0.7000000000000001, 0.09999999999999998]\n",
            "Corresponding dev minimum perplexity: 247.2466910211572\n",
            "Test perplexity: 244.09669707439983\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def perplexity_of_interpolated_model(lambdas, unigram_lm, bigram_lm, trigram_lm, sentences):\n",
        "    perplexity = 0\n",
        "    total_words = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence_perplexity = 1\n",
        "        for i in range(len(sentence)):\n",
        "            # calculate the probability for each n-gram model\n",
        "            unigram_prob = unigram_lm.score(sentence[i])\n",
        "            bigram_prob = bigram_lm.score(sentence[i], sentence[max(i - 1, 0):i])\n",
        "            trigram_prob = trigram_lm.score(sentence[i], sentence[max(i - 2, 0):i])\n",
        "            # interpolate the probabilities\n",
        "            interpolated_prob = lambdas[0] * unigram_prob + lambdas[1] * bigram_prob + lambdas[2] * trigram_prob\n",
        "            sentence_perplexity *= (1 / interpolated_prob)\n",
        "        \n",
        "        total_words += len(sentence)\n",
        "        perplexity += np.log(sentence_perplexity)\n",
        "    \n",
        "    return np.exp(perplexity / total_words)\n",
        "\n",
        "# Find the coefficients that minimize the perplexity\n",
        "best_lambdas = None\n",
        "best_perplexity = float('inf')\n",
        "\n",
        "# print(np.arange(0, 1.1, 0.1))\n",
        "for lambda1 in np.arange(0, 1.1, 0.05):\n",
        "    for lambda2 in np.arange(0, 1.1 - lambda1, 0.05):\n",
        "        lambda3 = 1 - lambda1 - lambda2\n",
        "        lambdas = [lambda1, lambda2, lambda3]\n",
        "\n",
        "        current_perplexity = perplexity_of_interpolated_model(lambdas, unigram_lm, bigram_lm, trigram_lm, dev_sentences)\n",
        "        \n",
        "        if current_perplexity < best_perplexity:\n",
        "            best_perplexity = current_perplexity\n",
        "            best_lambdas = lambdas\n",
        "\n",
        "print(\"Best coefficients:\", best_lambdas)\n",
        "print(\"Corresponding dev minimum perplexity:\", best_perplexity)\n",
        "\n",
        "test_perplexity = perplexity_of_interpolated_model(best_lambdas, unigram_lm, bigram_lm, trigram_lm, test_sentences)\n",
        "print(\"Test perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKqmQ37lcH2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "Since add k smoothing (which is Lidstone model fron nltk) provides the best result among all smoothing attempts above, we build 1~3gram models that applies add k smoothing with the same k value from above. Then, I tried to use the stepsize of 0.1 and 0.05 for probing the best combination of n-gram combinations. The perplexity was lowered the most when we assign a relatively large weight to the bigram model on the validation and test sets.\n",
        "Moreover, although nltk has perplexity function, the behavior looks weird, leading to very unbalanced weight distribution. Thus, I have computed the perplexity sentence by sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzSbk2bClf3u"
      },
      "source": [
        "##### **Optimization**:\n",
        "\n",
        "\\# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTcTlLuloHu"
      },
      "source": [
        "## 3 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "7jb0OQ-yltc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-22 14:46:02--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
            "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度：210427 (205K) [text/plain]\n",
            "正在保存至: “dev.in”\n",
            "\n",
            "dev.in              100%[===================>] 205.50K  --.-KB/s  用时 0.1s      \n",
            "\n",
            "2023-10-22 14:46:03 (1.51 MB/s) - 已保存 “dev.in” [210427/210427])\n",
            "\n",
            "--2023-10-22 14:46:03--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.out\n",
            "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度：10018 (9.8K) [text/plain]\n",
            "正在保存至: “dev.out”\n",
            "\n",
            "dev.out             100%[===================>]   9.78K  --.-KB/s  用时 0.001s    \n",
            "\n",
            "2023-10-22 14:46:03 (14.3 MB/s) - 已保存 “dev.out” [10018/10018])\n",
            "\n",
            "--2023-10-22 14:46:03--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/test.in\n",
            "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度：68304 (67K) [text/plain]\n",
            "正在保存至: “test.in”\n",
            "\n",
            "test.in             100%[===================>]  66.70K  --.-KB/s  用时 0.06s     \n",
            "\n",
            "2023-10-22 14:46:04 (1.04 MB/s) - 已保存 “test.in” [68304/68304])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.out\n",
        "!wget -O test.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/test.in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Using n gram models trained above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "def train_prep_language_model(n, train_data, vocab, lidstone_lambda=5e-3):\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # lm = MLE(n, vocabulary=vocab)\n",
        "    lm = Lidstone(lidstone_lambda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    return lm\n",
        "\n",
        "def process_PREP(tokens_unmerged):\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens_unmerged):\n",
        "        if tokens_unmerged[i:i+3] == ['<', 'PREP', '>']:\n",
        "            tokens.append('<PREP>')\n",
        "            i += 3\n",
        "        else:\n",
        "            tokens.append(tokens_unmerged[i])\n",
        "            i += 1\n",
        "    return tokens\n",
        "\n",
        "def preprocess_train_file_preposition(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.readlines()\n",
        "    # Tokenize and preprocess the sentences\n",
        "    sentences = []\n",
        "    unpaded_sentences = []\n",
        "    for line in content:\n",
        "        line = re.sub(f\"[{string.punctuation}]\", \"\", line)\n",
        "        tokens = nltk.word_tokenize(line.strip())\n",
        "        tokens = list(pad_both_ends(tokens, n=2))  # Pad the sentence with <s> and </s>\n",
        "        sentences.append(tokens)\n",
        "    # Create a frequency dictionary using nltk's FreqDist\n",
        "    freq_dict = nltk.FreqDist(token for tokens in sentences for token in tokens)\n",
        "    # print(sorted(freq_dict))\n",
        "    vocab = Vocabulary(freq_dict, unk_cutoff=3)\n",
        "    return sentences, vocab\n",
        "\n",
        "def preprocess_file_preposition(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.readlines()\n",
        "    # Tokenize and preprocess the sentences\n",
        "    sentences = []\n",
        "    unpaded_sentences = []\n",
        "    for line in content:\n",
        "        # punctuation_to_remove = \"\".join([ch for ch in string.punctuation if ch not in \"<>\"])\n",
        "        # line = re.sub(f\"[{punctuation_to_remove}]\", \"\", line)\n",
        "        tokens = nltk.word_tokenize(line.strip())\n",
        "        tokens = list(pad_both_ends(tokens, n=2))  # Pad the sentence with <s> and </s>\n",
        "        sentences.append(tokens)\n",
        "    # Create a frequency dictionary using nltk's FreqDist\n",
        "    freq_dict = nltk.FreqDist(token for tokens in sentences for token in tokens)\n",
        "    # print(sorted(freq_dict))\n",
        "    vocab = Vocabulary(freq_dict, unk_cutoff=3)\n",
        "    return sentences, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev set accuracy: 0.5701262272089762\n"
          ]
        }
      ],
      "source": [
        "def evaluate_PREP_acc(sentences, answers, unigram_lm, bigram_lm, trigram_lm):\n",
        "    prepositions = [\"on\", \"in\", \"at\", \"for\", \"of\"]\n",
        "    # Initialize variables\n",
        "    total_masked = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    lambdas = [0.15, 0.7, 0.15] # best coefficients found in previous section\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        tokens = process_PREP(sentence)\n",
        "        masked_indices = [i for i, token in enumerate(tokens) if token == \"<PREP>\"]\n",
        "        guess_index = 0\n",
        "        for masked_index in masked_indices:\n",
        "            # Predict masked word using ngram score\n",
        "            max_prob = -1\n",
        "            prediction = None\n",
        "            for prep in prepositions:\n",
        "                # calculate the probability for each n-gram model that uses K smoothing\n",
        "                unigram_prob = unigram_lm.score(prep)\n",
        "                bigram_prob = bigram_lm.score(prep, tokens[max(masked_index - 1, 0):masked_index]) + bigram_lm.score(prep, tokens[masked_index:min(masked_index + 1, len(tokens))])\n",
        "                trigram_prob = trigram_lm.score(prep, tokens[max(masked_index - 2, 0):masked_index]) + trigram_lm.score(prep, tokens[masked_index:min(masked_index + 2, len(tokens))])\n",
        "                # interpolate the probabilities\n",
        "                prob = lambdas[0] * unigram_prob + lambdas[1] * bigram_prob + lambdas[2] * trigram_prob\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    prediction = prep\n",
        "            \n",
        "            # Compare prediction with the answer\n",
        "            total_masked += 1\n",
        "            answer = answers[i].split()\n",
        "            # print(f\"Prediction for <PREP> at index {masked_index}:\", prediction)\n",
        "            # print(f\"Answer for <PREP> at index {masked_index}:\", answer[guess_index])\n",
        "            if prediction == answer[guess_index]:\n",
        "                correct_predictions += 1\n",
        "            guess_index += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions / total_masked\n",
        "    return accuracy\n",
        "\n",
        "def save_prediction(sentences, unigram_lm, bigram_lm, trigram_lm):\n",
        "    prepositions = [\"on\", \"in\", \"at\", \"for\", \"of\"]\n",
        "    lambdas = [0.15, 0.7, 0.15] # best coefficients found in previous section\n",
        "\n",
        "    # Open the output file\n",
        "    with open('test.out', 'w') as outfile:\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            tokens = process_PREP(sentence)\n",
        "            masked_indices = [i for i, token in enumerate(tokens) if token == \"<PREP>\"]\n",
        "            predictions = []\n",
        "            for masked_index in masked_indices:\n",
        "                # Predict masked word using ngram score\n",
        "                max_prob = -1\n",
        "                prediction = None\n",
        "                for prep in prepositions:\n",
        "                    # calculate the probability for each n-gram model that uses K smoothing\n",
        "                    unigram_prob = unigram_lm.score(prep)\n",
        "                    bigram_prob = bigram_lm.score(prep, tokens[max(masked_index - 1, 0):masked_index]) + bigram_lm.score(prep, tokens[masked_index:min(masked_index + 1, len(tokens))])\n",
        "                    trigram_prob = trigram_lm.score(prep, tokens[max(masked_index - 2, 0):masked_index]) + trigram_lm.score(prep, tokens[masked_index:min(masked_index + 2, len(tokens))])\n",
        "                    # interpolate the probabilities\n",
        "                    prob = lambdas[0] * unigram_prob + lambdas[1] * bigram_prob + lambdas[2] * trigram_prob\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        prediction = prep\n",
        "\n",
        "                # Store the prediction\n",
        "                predictions.append(prediction)\n",
        "            # Write the predictions to the output file\n",
        "            outfile.write(' '.join(predictions) + '\\n')\n",
        "    # Calculate accuracy\n",
        "\n",
        "# train_source = './data/prep/validate.in' # this is the same as dev.in except that it has answers inserted\n",
        "dev_source = './dev.in'\n",
        "dev_answer = './dev.out'\n",
        "test_source = './test.in'\n",
        "# test_answer = './data/prep/test.out'\n",
        "\n",
        "# Load and tokenize source sentences\n",
        "# train_sentences, train_vocab = preprocess_train_file_preposition(train_source) # unused\n",
        "dev_sentences, dev_vocab = preprocess_file_preposition(dev_source)  \n",
        "test_sentences, test_vocab = preprocess_file_preposition(test_source)\n",
        "\n",
        "# Load the answers\n",
        "with open(dev_answer, \"r\") as f:\n",
        "    dev_answers = f.read().splitlines()\n",
        "# with open(test_answer, \"r\") as f:\n",
        "#     test_answers = f.read().splitlines()\n",
        "\n",
        "print(\"Dev set accuracy:\", evaluate_PREP_acc(dev_sentences, dev_answers, unigram_lm, bigram_lm, trigram_lm))\n",
        "# print(\"Test set accuracy:\", evaluate_PREP_acc(test_sentences, test_answers, unigram_lm, bigram_lm, trigram_lm))\n",
        "save_prediction(test_sentences, unigram_lm, bigram_lm, trigram_lm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion\n",
        "\n",
        "for this task, I have applied both k smoothing and interpolation to the 3 ngram models. Moreover, adding the reversed context helped improve the accuracy by a little bit, since guessing the preposition while also using reversed context provides more information about the preposition. Unlike the equal condition while guessing the whole sentences, it do help improve the accuracy when we are guessing single word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 RoBERTa Attempt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is just playing for fun, testing if transformers can provide better results. If the local files are not found, just ignore these code blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_input_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    text = text.replace('<PREP>', '<mask>')\n",
        "    with open(\"./data/prep/dev_mask.in\", 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "input_file = \"./data/prep/dev.in\"\n",
        "preprocessed_file = \"./data/prep/dev_mask.in\"\n",
        "preprocess_input_file(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 03:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling, RobertaForMaskedLM, Trainer, TrainingArguments, pipeline\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=preprocessed_file,\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=1.0\n",
        ")\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./fine_tuned_roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['on', 'of', 'in', 'in']\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = RobertaForMaskedLM.from_pretrained(\"../A1/fine_tuned_roberta\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Create a fill-mask pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=fine_tuned_model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Predict masked words\n",
        "sentence = \"palestinian leader yasser arafat <mask> wednesday welcomed the resumption <mask> israeli-syrian peace talks , which were due to begin later <mask> the day <mask> the united states.\"\n",
        "masked_sentence = sentence.replace('<PREP>', '<mask>')\n",
        "predictions = fill_mask(masked_sentence)\n",
        "# print(predictions)\n",
        "\n",
        "# Filter and print preposition predictions\n",
        "prepositions = [\"at\", \"in\", \"on\", \"for\", \"of\"]\n",
        "\n",
        "preposition_predictions = []\n",
        "for mask_preds in predictions:\n",
        "    for pred in mask_preds:\n",
        "        token = pred['token_str'].strip()\n",
        "        if token in prepositions:\n",
        "            preposition_predictions.append(token)\n",
        "            break\n",
        "\n",
        "print(preposition_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correction rate: 76.90%\n"
          ]
        }
      ],
      "source": [
        "with open(\"./data/prep/test.in\", \"r\", encoding=\"utf-8\") as f:\n",
        "    input = f.readlines()\n",
        "    test_sentences = []\n",
        "    for line in input:\n",
        "        test_sentences.append(line.replace('<PREP>', '<mask>'))\n",
        "        \n",
        "with open(\"./data/prep/test.out\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_answer = f.readlines()\n",
        "\n",
        "# print(test_sentences)\n",
        "# print(test_out_lines)\n",
        "\n",
        "def get_predictions(sentence):\n",
        "    preposition_predictions = []\n",
        "    predictions = fill_mask(sentence)\n",
        "    for pred in predictions:\n",
        "        if type(pred) is dict:\n",
        "            token = pred['token_str'].strip()\n",
        "            if token in prepositions:\n",
        "                preposition_predictions.append(token)\n",
        "                break\n",
        "        else:\n",
        "            for candidate in pred:\n",
        "                token = candidate['token_str'].strip()\n",
        "                if token in prepositions:\n",
        "                    preposition_predictions.append(token)\n",
        "                    break\n",
        "    return preposition_predictions\n",
        "\n",
        "# Evaluate the model's performance\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for test_in_line, test_out_line in zip(test_sentences, test_answer):\n",
        "    predictions = get_predictions(test_in_line)\n",
        "    correct_answers = test_out_line.strip().split()\n",
        "    total_count += len(correct_answers)\n",
        "\n",
        "    for pred, correct in zip(predictions, correct_answers):\n",
        "        if pred == correct:\n",
        "            correct_count += 1\n",
        "\n",
        "# Calculate and print the correction rate\n",
        "correction_rate = correct_count / total_count\n",
        "print(f\"Correction rate: {correction_rate:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
