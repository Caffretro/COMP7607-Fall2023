{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd-mtF0PTrv"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c9YBduQCI4"
      },
      "source": [
        "## 1 $n$-gram Language Model\n",
        "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) {=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ_Z1g8QZef"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "$ p(\\vec{w})$ =  $ p(w_1) â‹… p(w_2 \\mid w_1) â‹… p(w_3 \\mid w_1, w_2) â‹…(w_4 \\mid w_2, w_3) ... p(w_n \\mid w_{n-2}, w_{n-1})$ \\\\\n",
        "$ = \\dfrac{C(w_1)}{C(*)} \\dfrac{C(w_1~w_2)}{C(w_1)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgExbf1QtCH"
      },
      "source": [
        "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
        "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
        "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1ZGFIaRPCP"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "The MLE of $ p(\\vec{w})$ is $ C(w_1) \\dfrac{C(w_1~w_2)}{C(w_1)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$ \\\\\n",
        "\n",
        "Which can be canceled to: \\\\\n",
        "\n",
        "$ C(w_1~w_2~w_3) \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$ \\\\\n",
        "\n",
        "Similarly, we can write the MLE of $ p_{reversed}(\\vec{w}) $ to: $ C(w_n) \\dfrac{C(w_{n-1}~w_n)}{C(w_n)} \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-1}~w_n)} \\dfrac{C(w_{n-3}~w_{n-2}~w_{n-1})}{C(w_{n-2}~w_{n-1})} ... \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} $ \\\\\n",
        "\n",
        "Which can also be canceled to: \\\\\n",
        "\n",
        "$ {C(w_{n-2} ~w_{n-1} ~w_n)} \\dfrac{C(w_{n-3}~w_{n-2}~w_{n-1})}{C(w_{n-2}~w_{n-1})} ... \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} $ \\\\\n",
        "\n",
        "This is equavalent to the answer above, if we multiply all the denominators and numerators respectively, the composition are equavalent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEc5kz4RniG"
      },
      "source": [
        "## 2 $N$-gram Language Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kSwtN79jWgp"
      },
      "outputs": [],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9HCVQwqkTc_"
      },
      "source": [
        "### 2.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhFKCzwkaTn"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<s>', 'facebook', 'has', 'released', 'a', 'report', 'that', 'shows', 'what', 'content', 'was', 'most', 'widely', 'viewed', 'by', 'americans', 'between', 'april', 'and', 'june', '.', '</s>'], ['<s>', 'it', 'contains', 'sections', 'showing', 'the', 'top', '20', 'domains', ',', 'links', ',', 'pages', ',', 'and', 'posts', 'in', 'terms', 'of', 'views', '.', '</s>'], ['<s>', 'a', 'companion', 'guide', 'that', 'describes', 'how', 'the', 'data', 'was', 'gathered', 'and', 'analyzed', 'was', 'also', 'released', '.', '</s>'], ['<s>', 'facebook', 'released', 'the', 'data', 'in', 'response', 'to', 'reports', 'that', 'posts', 'from', 'right-wing', 'sources', 'had', 'the', 'most', 'interaction', '.', '</s>'], ['<s>', 'the', 'top', 'posts', 'only', 'account', 'for', 'less', 'than', '0', '.', '</s>'], ['<s>', '1', 'percent', 'of', 'the', 'content', 'viewed', 'by', 'us', 'users', ',', 'and', 'the', 'data', 'only', 'accounts', 'for', 'public', 'posts', ',', 'not', 'posts', 'made', 'in', 'private', 'groups', '.', '</s>'], ['<s>', 'a', 'link', 'to', 'the', 'report', 'is', 'available', 'in', 'the', 'article', '.', '</s>'], ['<s>', 'a', 'quantum', 'effect', 'called', 'superabsorption', 'allows', 'a', 'collection', 'of', 'molecules', 'to', 'absorb', 'light', 'more', 'efficiently', 'than', 'if', 'each', 'molecule', 'were', 'acting', 'individually', '.', '</s>'], ['<s>', 'researchers', 'have', 'used', 'this', 'phenomenon', 'to', 'create', 'a', 'proof-of-concept', 'quantum', 'battery', '.', '</s>'], ['<s>', 'the', 'larger', 'the', 'battery', 'was', ',', 'the', 'faster', 'it', 'charged', ',', 'demonstrating', 'superabsorption', 'at', 'work', '.', '</s>']]\n",
            "[('the', 56771), ('<s>', 50000), ('</s>', 50000), ('.', 49845), (',', 38772), ('to', 31878), ('a', 31117), ('of', 25486), ('and', 22499), ('in', 22468)]\n",
            "[['<s>', 'bush', 'has', 'admitted', 'a', 'familiarity', 'with', 'london', \"'s\", 'pubs', 'ahead', 'of', 'his', 'arrival', 'in', 'the', 'city', 'this', 'week', 'for', 'a', 'state', 'visit', 'to', 'britain', '.', '</s>'], ['<s>', 'the', 'death', 'toll', 'in', 'the', 'collapse', 'of', 'a', 'walkway', 'on', 'the', 'queen', 'mary', 'ii', 'has', 'risen', 'to', ',', 'according', 'to', 'a', 'new', 'toll', 'issued', 'sunday', 'by', 'a', 'spokesman', 'for', 'alstom', ',', 'which', 'owns', 'the', 'shipyard', '.', '</s>'], ['<s>', 'the', 'match', 'receipts', 'from', 'sunday', \"'s\", 'friendly', 'between', 'italy', 'and', 'romania', 'will', 'go', 'to', 'the', 'families', 'of', 'those', 'who', 'died', 'in', 'this', 'week', \"'s\", 'bomb', 'attack', 'on', 'an', 'italian', 'police', 'base', 'in', 'iraq', '.', '</s>'], ['<s>', 'south', 'korean', 'foreign', 'and', 'trade', 'minister', 'yun', 'young-kwan', 'was', 'due', 'in', 'moscow', 'late', 'sunday', 'for', 'a', 'three-day', 'visit', 'to', 'discuss', 'a', 'broad', 'range', 'of', 'regional', 'and', 'bilateral', 'issues', ',', 'a', 'south', 'korean', 'embassy', 'official', 'said', '.', '</s>'], ['<s>', 'points', 'on', 'the', 'richter', 'scale', 'occurred', 'off', 'the', 'western', 'greek', 'island', 'of', 'cephallonia', 'early', 'on', 'sunday', ',', 'the', 'athens', 'observatory', 'said', '.', '</s>'], ['<s>', 'a', 'french', 'woman', 'working', 'for', 'the', 'un', \"'s\", 'refugee', 'agency', 'unhcr', 'was', 'shot', 'dead', 'by', 'unknown', 'gunmen', 'sunday', 'afternoon', 'in', 'the', 'southeast', 'afghan', 'town', 'of', 'ghazni', ',', 'a', 'un', 'spokesman', 'said', '.', '</s>'], ['<s>', 'at', 'least', 'people', 'were', 'killed', 'in', 'bomb', 'attacks', 'at', 'two', 'synagogues', 'in', 'the', 'historic', 'turkish', 'city', 'of', 'istanbul', 'on', 'saturday', ',', 'the', 'city', \"'s\", 'health', 'department', 'said', 'sunday', '.', '</s>'], ['<s>', 'pope', 'john', 'paul', 'ii', 'criticised', 'israel', 'on', 'sunday', 'for', 'building', 'a', 'barrier', 'in', 'the', 'west', 'bank', ',', 'saying', 'the', 'middle', 'east', '``', 'does', 'not', 'need', 'walls', 'but', 'bridges', '.', '</s>'], ['<s>', 'a', 'french', 'woman', 'working', 'for', 'the', 'un', \"'s\", 'high', 'commissioner', 'for', 'refugees', '-lrb-', 'unhcr', '-rrb-', 'was', 'shot', 'dead', 'by', 'unknown', 'gunmen', 'sunday', 'afternoon', 'in', 'the', 'southeast', 'afghan', 'town', 'of', 'ghazni', ',', 'a', 'un', 'spokesman', 'said', '.', '</s>'], ['<s>', 'president', 'chandrika', 'kumaratunga', 'has', 'denied', 'reports', 'she', 'has', 'offered', 'to', 'share', 'the', 'vital', 'defense', 'portfolio', 'with', 'her', 'estranged', 'prime', 'minister', ',', 'her', 'spokesman', 'peiris', 'said', 'sunday', '.', '</s>']]\n",
            "[('the', 7375), ('<s>', 5000), ('</s>', 5000), ('.', 4997), (',', 4548), ('a', 3985), ('of', 3726), ('in', 3655), ('to', 3424), ('on', 2396)]\n",
            "[['<s>', 'britain', \"'s\", 'queen', 'mother', 'walked', 'out', 'of', 'a', 'hospital', 'unassisted', 'monday', ',', 'days', 'after', 'undergoing', 'hip', 'replacement', 'surgery', '.', '</s>'], ['<s>', 'lebanon', 'is', 'seeking', 'the', 'extradition', 'from', 'sweden', 'of', 'an', 'alleged', 'accomplice', 'in', 'the', 'february', 'bomb', 'attack', 'on', 'a', 'church', 'north', 'of', 'beirut', 'that', 'killed', 'people', ',', 'legal', 'officials', 'said', 'monday', '.', '</s>'], ['<s>', 'israeli', 'prime', 'minister', 'shimon', 'peres', 'will', 'meet', 'plo', 'leader', 'yasser', 'arafat', 'on', 'friday', 'for', 'the', 'first', 'time', 'since', 'succeeding', 'yitzhak', 'rabin', ',', 'a', 'plo', 'official', 'said', '.', '</s>'], ['<s>', 'the', 'host', 'stadium', 'for', 'the', 'world', 'cup', 'final', 'will', 'be', 'named', 'the', 'stade', 'de', 'france', ',', 'sports', 'minister', 'guy', 'drut', 'announced', 'monday', 'after', 'months', 'of', 'deliberations', '.', '</s>'], ['<s>', 'a', 'second', 'nato', 'plane', 'landed', 'in', 'the', 'bosnian', 'capital', 'on', 'monday', 'carrying', 'some', 'soldiers', 'from', 'the', 'united', 'states', ',', 'britain', ',', 'france', 'and', 'belgium', ',', 'part', 'of', 'the', 'advance', 'troops', 'preparing', 'for', 'the', 'deployment', 'of', 'a', ',', '-strong', 'nato-led', 'peace', 'force', 'for', 'the', 'former', 'yugoslavia', '.', '</s>'], ['<s>', 'turkey', 'said', 'monday', 'that', 'it', 'would', 'contribute', 'up', 'to', ',', 'troops', 'to', 'the', 'nato-led', 'force', 'being', 'set', 'up', 'to', 'enforce', 'a', 'peace', 'plan', 'in', 'war-torn', 'bosnia', '.', '</s>'], ['<s>', 'london', \"'s\", 'oil', 'market', 'stayed', 'calm', 'in', 'early', 'trading', 'monday', ',', 'after', 'sunday', \"'s\", 'announcement', 'that', 'the', 'health', 'of', 'king', 'fahd', 'of', 'saudi', 'arabia', ',', 'the', 'world', \"'s\", 'leading', 'exporter', 'of', 'crude', ',', 'had', 'deteriorated', '.', '</s>'], ['<s>', 'swedish', 'forestry', 'group', \"'s\", 'pulp', 'division', ',', 'cell', ',', 'cut', 'its', 'pulp', 'price', 'from', ',', 'to', 'dollars', 'per', 'tonne', 'effective', 'from', 'december', ',', 'the', 'group', 'said', 'monday', '.', '</s>'], ['<s>', 'share', 'prices', 'here', 'were', 'showing', 'a', 'hefty', 'loss', 'of', '.', '</s>'], ['<s>', 'points', 'on', 'the', 'cac', 'index', 'in', 'mid-afternoon', 'monday', 'as', 'strikes', 'in', 'the', 'public', 'sector', 'began', 'to', 'spread', 'to', 'the', 'private', 'sector', '.', '</s>']]\n",
            "[('the', 7799), (',', 5164), ('<s>', 5000), ('.', 5000), ('</s>', 5000), ('a', 4058), ('of', 3964), ('to', 3533), ('in', 3488), ('on', 2465)]\n",
            "Train vocabulary size: 17658\n",
            "Dev vocabulary size: 4923\n",
            "Test vocabulary size: 4679\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "def preprocess_file_nltk(file_path, min_freq=3):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.readlines()\n",
        "\n",
        "    # Tokenize and preprocess the sentences\n",
        "    sentences = []\n",
        "    unpaded_sentences = []\n",
        "    for line in content:\n",
        "        tokens = nltk.word_tokenize(line.strip())\n",
        "        unpaded_sentences.append(tokens)\n",
        "        tokens = list(pad_both_ends(tokens, n=2))  # Pad the sentence with <s> and </s>\n",
        "        sentences.append(tokens)\n",
        "    print(sentences[:10])\n",
        "    # Create a frequency dictionary using nltk's FreqDist\n",
        "    freq_dict = nltk.FreqDist(token for tokens in sentences for token in tokens)\n",
        "    print(freq_dict.most_common(10))\n",
        "    # only keep tokens that appear at least 3 times in the file first \n",
        "    vocab = Vocabulary(freq_dict, unk_cutoff=min_freq)\n",
        "    return sentences, unpaded_sentences, vocab\n",
        "\n",
        "train_file = './data/lm/train.txt'\n",
        "dev_file = './data/lm/dev.txt'\n",
        "test_file = './data/lm/test.txt'\n",
        "\n",
        "train_sentences, train_sentences_unpad, train_vocab = preprocess_file_nltk(train_file)\n",
        "dev_sentences, dev_sentences_unpad, dev_vocab = preprocess_file_nltk(dev_file)\n",
        "test_sentences, test_sentences_unpad, test_vocab = preprocess_file_nltk(test_file)\n",
        "\n",
        "print(f\"Train vocabulary size: {len(train_vocab)}\")\n",
        "print(f\"Dev vocabulary size: {len(dev_vocab)}\")\n",
        "print(f\"Test vocabulary size: {len(test_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQNsUA5kfZe"
      },
      "source": [
        "**Discussion**\n",
        "Given the vocabulary size above, the number of parameters in a n-gram model is simply vocab size to its nth times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzDNMVikkeX"
      },
      "source": [
        "### 2.2 $N$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxkcs2HykuR2"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=3 unk_label='<UNK>' and 17658 items>\n",
            "<Vocabulary with cutoff=3 unk_label='<UNK>' and 17658 items>\n",
            "Unigram Model Perplexity: 835.112106023236\n",
            "Bigram Model Perplexity: inf\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from nltk import everygrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten\n",
        "\n",
        "def train_language_model(n, train_data, vocab):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    # Train the language model\n",
        "    lm = MLE(n, vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    print(lm.vocab)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "def compute_perplexity(n, lm, test_data):\n",
        "    # Preprocess the test data\n",
        "    padded_sentences = [list(pad_both_ends(sent, n)) for sent in test_data]\n",
        "    # flattened_sentences = list(flatten(test_data))\n",
        "    # test_data, padded_sents = padded_everygram_pipeline(n, test_data)\n",
        "    test_ngrams = [ngram for sent in test_data for ngram in everygrams(sent, max_len=n)]\n",
        "    # Calculate perplexity\n",
        "    return lm.perplexity(test_ngrams)\n",
        "\n",
        "# Train models and calculate perplexity\n",
        "unigram_lm = train_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_language_model(2, train_sentences, train_vocab)\n",
        "\n",
        "unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "print(\"Unigram Model Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Model Perplexity:\", bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3o9Nez8kvYm"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "The infinity from Bigram model comes from a 0 numerator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQUqM73kzf-"
      },
      "source": [
        "### 2.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LgXRmJwk3Y-"
      },
      "source": [
        "#### 2.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG7jCIRk7Qw"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Perplexity: 836.0150916297277\n",
            "Bigram Perplexity: 1006.1865977200891\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm import Laplace\n",
        "\n",
        "def train_laplace_language_model(n, train_data, vocab):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    # Train the language model with Laplace smoothing\n",
        "    lm = Laplace(n, vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "# Train the models with Laplace smoothing\n",
        "unigram_lm = train_laplace_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_laplace_language_model(2, train_sentences, train_vocab)\n",
        "\n",
        "# Calculate perplexity\n",
        "unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "print(\"Unigram Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Perplexity:\", bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yTKPXFk8f2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cFbczqlBR_"
      },
      "source": [
        "#### 2.3.2: Add-$k$ smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_ZiAgIlPUu"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Perplexity: 835.1135795103045\n",
            "Bigram Perplexity: 495.6881461545059\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm import Lidstone\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def train_lidstone_language_model(n, train_data, vocab, lmda):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    # Train the language model with Lidstone smoothing\n",
        "    lm = Lidstone(lmda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "# Train the models with Lidstone smoothing (choose a value for lambda, e.g., 0.5)\n",
        "lmda = 5e-3\n",
        "# Train the models with Laplace smoothing\n",
        "unigram_lm = train_lidstone_language_model(1, train_sentences, train_vocab, lmda)\n",
        "bigram_lm = train_lidstone_language_model(2, train_sentences, train_vocab, lmda)\n",
        "\n",
        "# Calculate perplexity\n",
        "unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "print(\"Unigram Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Perplexity:\", bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFNf8OIlQ0O"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKEO_TqlUrX"
      },
      "source": [
        "#### 2.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdd4cvYlZuO"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training language models\n",
            "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
            "Finished training hyperparameters\n",
            "Best Lambdas: [1.0, 0.1, -0.1]\n",
            "Training Perplexity: 910.0259141168684\n",
            "Dev Perplexity: 903.3261965637678\n",
            "Test Perplexity: 859.5304041913109\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "# Train MLE models with different n-grams\n",
        "def train_mle_language_model(n, train_data, vocab, lidstone_lambda=5e-3):\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # lm = MLE(n, vocabulary=vocab)\n",
        "    lm = Lidstone(lidstone_lambda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    return lm\n",
        "\n",
        "def interpolated_probability(ngram, lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm):\n",
        "    unigram_prob = unigram_lm.score(ngram[-1])\n",
        "    bigram_prob = bigram_lm.score(ngram[-2:], ngram[:-1])\n",
        "    trigram_prob = trigram_lm.score(ngram, ngram[:-1])\n",
        "\n",
        "    return lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
        "\n",
        "def interpolated_perplexity(lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm, dataset):\n",
        "    log_prob_sum = 0\n",
        "    token_count = 0\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    for sentence in dataset:\n",
        "        for i in range(2, len(sentence)):\n",
        "            ngram = tuple(sentence[i-2:i+1])\n",
        "            prob = interpolated_probability(ngram, lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm)\n",
        "            log_prob_sum += math.log(prob + epsilon) \n",
        "            token_count += 1\n",
        "\n",
        "    return math.exp(-log_prob_sum / token_count)\n",
        "\n",
        "# Train unigram, bigram, and trigram models\n",
        "unigram_lm = train_mle_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_mle_language_model(2, train_sentences, train_vocab)\n",
        "trigram_lm = train_mle_language_model(3, train_sentences, train_vocab)\n",
        "print(\"Finished training language models\")\n",
        "models = [unigram_lm, bigram_lm, trigram_lm]\n",
        "\n",
        "# in interpolation, we always mix results of all models with trained lambda values from dev set\n",
        "# Optimize hyperparameters (lambdas) on the dev set\n",
        "best_lambdas = [0, 0, 0]\n",
        "print(np.arange(0, 1.1, 0.1))\n",
        "best_dev_perplexity = float('inf')\n",
        "for lambda1 in np.arange(0, 1.1, 0.1):\n",
        "    for lambda2 in np.arange(0, 1.1 - lambda1, 0.1):\n",
        "        lambda3 = 1 - lambda1 - lambda2\n",
        "\n",
        "        perplexity = interpolated_perplexity(lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm, dev_sentences)\n",
        "        if perplexity < best_dev_perplexity:\n",
        "            best_dev_perplexity = perplexity\n",
        "            best_lambdas[0], best_lambdas[1], best_lambdas[2] = lambda1, lambda2, lambda3\n",
        "\n",
        "print(\"Finished training hyperparameters\")\n",
        "\n",
        "# Report perplexity on the training and dev sets\n",
        "# train_perplexity = compute_interpolated_perplexity(models, best_lambdas, train_sentences)\n",
        "train_perplexity = interpolated_perplexity(best_lambdas[0], best_lambdas[1], best_lambdas[2], unigram_lm, bigram_lm, trigram_lm, train_sentences)\n",
        "print(\"Best Lambdas:\", best_lambdas)\n",
        "print(\"Training Perplexity:\", train_perplexity)\n",
        "print(\"Dev Perplexity:\", best_dev_perplexity)\n",
        "\n",
        "# Report perplexity on the test set\n",
        "test_perplexity = interpolated_perplexity(best_lambdas[0], best_lambdas[1], best_lambdas[2], unigram_lm, bigram_lm, trigram_lm, test_sentences)\n",
        "print(\"Test Perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKqmQ37lcH2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzSbk2bClf3u"
      },
      "source": [
        "##### **Optimization**:\n",
        "\n",
        "\\# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTcTlLuloHu"
      },
      "source": [
        "## 3 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jb0OQ-yltc3"
      },
      "outputs": [],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://github.com/qtli/COMP7607-Fall2023/blob/master/assignments/A1/data/prep/dev.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 RoBERTa Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_input_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    text = text.replace('<PREP>', '<mask>')\n",
        "    with open(\"./data/prep/dev_mask.in\", 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "input_file = \"./data/prep/dev.in\"\n",
        "preprocessed_file = \"./data/prep/dev_mask.in\"\n",
        "preprocess_input_file(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 03:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling, RobertaForMaskedLM, Trainer, TrainingArguments, pipeline\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=preprocessed_file,\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=1.0\n",
        ")\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./fine_tuned_roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['on', 'of', 'in', 'in']\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = RobertaForMaskedLM.from_pretrained(\"../A1/fine_tuned_roberta\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Create a fill-mask pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=fine_tuned_model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Predict masked words\n",
        "sentence = \"palestinian leader yasser arafat <mask> wednesday welcomed the resumption <mask> israeli-syrian peace talks , which were due to begin later <mask> the day <mask> the united states.\"\n",
        "masked_sentence = sentence.replace('<PREP>', '<mask>')\n",
        "predictions = fill_mask(masked_sentence)\n",
        "# print(predictions)\n",
        "\n",
        "# Filter and print preposition predictions\n",
        "prepositions = [\"at\", \"in\", \"on\", \"for\", \"of\"]\n",
        "\n",
        "preposition_predictions = []\n",
        "for mask_preds in predictions:\n",
        "    for pred in mask_preds:\n",
        "        token = pred['token_str'].strip()\n",
        "        if token in prepositions:\n",
        "            preposition_predictions.append(token)\n",
        "            break\n",
        "\n",
        "print(preposition_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correction rate: 76.90%\n"
          ]
        }
      ],
      "source": [
        "with open(\"./data/prep/test.in\", \"r\", encoding=\"utf-8\") as f:\n",
        "    input = f.readlines()\n",
        "    test_sentences = []\n",
        "    for line in input:\n",
        "        test_sentences.append(line.replace('<PREP>', '<mask>'))\n",
        "        \n",
        "with open(\"./data/prep/test.out\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_answer = f.readlines()\n",
        "\n",
        "# print(test_sentences)\n",
        "# print(test_out_lines)\n",
        "\n",
        "def get_predictions(sentence):\n",
        "    preposition_predictions = []\n",
        "    predictions = fill_mask(sentence)\n",
        "    for pred in predictions:\n",
        "        if type(pred) is dict:\n",
        "            token = pred['token_str'].strip()\n",
        "            if token in prepositions:\n",
        "                preposition_predictions.append(token)\n",
        "                break\n",
        "        else:\n",
        "            for candidate in pred:\n",
        "                token = candidate['token_str'].strip()\n",
        "                if token in prepositions:\n",
        "                    preposition_predictions.append(token)\n",
        "                    break\n",
        "    return preposition_predictions\n",
        "\n",
        "# Evaluate the model's performance\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for test_in_line, test_out_line in zip(test_sentences, test_answer):\n",
        "    predictions = get_predictions(test_in_line)\n",
        "    correct_answers = test_out_line.strip().split()\n",
        "    total_count += len(correct_answers)\n",
        "\n",
        "    for pred, correct in zip(predictions, correct_answers):\n",
        "        if pred == correct:\n",
        "            correct_count += 1\n",
        "\n",
        "# Calculate and print the correction rate\n",
        "correction_rate = correct_count / total_count\n",
        "print(f\"Correction rate: {correction_rate:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
