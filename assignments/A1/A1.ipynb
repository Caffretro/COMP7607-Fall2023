{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd-mtF0PTrv"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c9YBduQCI4"
      },
      "source": [
        "## 1 $n$-gram Language Model\n",
        "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) {=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ_Z1g8QZef"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "$ p(\\vec{w})$ =  $ p(w_1) â‹… p(w_2 \\mid w_1) â‹… p(w_3 \\mid w_1, w_2) â‹…(w_4 \\mid w_2, w_3) ... p(w_n \\mid w_{n-2}, w_{n-1})$ \\\\\n",
        "$ = \\dfrac{C(w_1)}{C(*)} \\dfrac{C(w_1~w_2)}{C(w_1)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgExbf1QtCH"
      },
      "source": [
        "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
        "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
        "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1ZGFIaRPCP"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "The MLE of $ p(\\vec{w})$ is $ C(w_1) \\dfrac{C(w_1~w_2)}{C(w_1)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$ \\\\\n",
        "\n",
        "Which can be canceled to: \\\\\n",
        "\n",
        "$ C(w_1~w_2~w_3) \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} ... \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-2}~w_{n-1})}$ \\\\\n",
        "\n",
        "Similarly, we can write the MLE of $ p_{reversed}(\\vec{w}) $ to: $ C(w_n) \\dfrac{C(w_{n-1}~w_n)}{C(w_n)} \\dfrac{C(w_{n-2} ~w_{n-1} ~w_n)}{C(w_{n-1}~w_n)} \\dfrac{C(w_{n-3}~w_{n-2}~w_{n-1})}{C(w_{n-2}~w_{n-1})} ... \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} $ \\\\\n",
        "\n",
        "Which can also be canceled to: \\\\\n",
        "\n",
        "$ {C(w_{n-2} ~w_{n-1} ~w_n)} \\dfrac{C(w_{n-3}~w_{n-2}~w_{n-1})}{C(w_{n-2}~w_{n-1})} ... \\dfrac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\dfrac{C(w_1~w_2~w_3)}{C(w_1~w_2)} $ \\\\\n",
        "\n",
        "This is equavalent to the answer above, if we multiply all the denominators and numerators respectively, the composition are equavalent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEc5kz4RniG"
      },
      "source": [
        "## 2 $N$-gram Language Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kSwtN79jWgp"
      },
      "outputs": [],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9HCVQwqkTc_"
      },
      "source": [
        "### 2.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhFKCzwkaTn"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['<s>', 'facebook', 'has', 'released', 'a', 'report', 'that', 'shows', 'what', 'content', 'was', 'most', 'widely', 'viewed', 'by', 'americans', 'between', 'april', 'and', 'june', '.', '</s>'], ['<s>', 'it', 'contains', 'sections', 'showing', 'the', 'top', '20', 'domains', ',', 'links', ',', 'pages', ',', 'and', 'posts', 'in', 'terms', 'of', 'views', '.', '</s>'], ['<s>', 'a', 'companion', 'guide', 'that', 'describes', 'how', 'the', 'data', 'was', 'gathered', 'and', 'analyzed', 'was', 'also', 'released', '.', '</s>'], ['<s>', 'facebook', 'released', 'the', 'data', 'in', 'response', 'to', 'reports', 'that', 'posts', 'from', 'right-wing', 'sources', 'had', 'the', 'most', 'interaction', '.', '</s>'], ['<s>', 'the', 'top', 'posts', 'only', 'account', 'for', 'less', 'than', '0', '.', '</s>'], ['<s>', '1', 'percent', 'of', 'the', 'content', 'viewed', 'by', 'us', 'users', ',', 'and', 'the', 'data', 'only', 'accounts', 'for', 'public', 'posts', ',', 'not', 'posts', 'made', 'in', 'private', 'groups', '.', '</s>'], ['<s>', 'a', 'link', 'to', 'the', 'report', 'is', 'available', 'in', 'the', 'article', '.', '</s>'], ['<s>', 'a', 'quantum', 'effect', 'called', '<UNK>', 'allows', 'a', 'collection', 'of', 'molecules', 'to', 'absorb', 'light', 'more', 'efficiently', 'than', 'if', 'each', 'molecule', 'were', 'acting', 'individually', '.', '</s>'], ['<s>', 'researchers', 'have', 'used', 'this', 'phenomenon', 'to', 'create', 'a', 'proof-of-concept', 'quantum', 'battery', '.', '</s>'], ['<s>', 'the', 'larger', 'the', 'battery', 'was', ',', 'the', 'faster', 'it', 'charged', ',', 'demonstrating', '<UNK>', 'at', 'work', '.', '</s>']]\n",
            "Train vocabulary size: 17658\n",
            "Dev vocabulary size: 4923\n",
            "Test vocabulary size: 4679\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "def preprocess_file_nltk(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.readlines()\n",
        "\n",
        "    # Tokenize and preprocess the sentences\n",
        "    sentences = []\n",
        "    unpaded_sentences = []\n",
        "    for line in content:\n",
        "        tokens = nltk.word_tokenize(line.strip())\n",
        "        unpaded_sentences.append(tokens)\n",
        "        tokens = list(pad_both_ends(tokens, n=2))  # Pad the sentence with <s> and </s>\n",
        "        sentences.append(tokens)\n",
        "    # print(sentences[:10])\n",
        "    # Create a frequency dictionary using nltk's FreqDist\n",
        "    freq_dict = nltk.FreqDist(token for tokens in sentences for token in tokens)\n",
        "    # print(sorted(freq_dict))\n",
        "    # only keep tokens that appear at least 3 times in the file first \n",
        "    vocab = Vocabulary(freq_dict, unk_cutoff=3)\n",
        "    return sentences, unpaded_sentences, vocab\n",
        "\n",
        "train_file = './data/lm/train.txt'\n",
        "dev_file = './data/lm/dev.txt'\n",
        "test_file = './data/lm/test.txt'\n",
        "\n",
        "train_sentences_noUNK, train_sentences_unpad_noUNK, train_vocab = preprocess_file_nltk(train_file)\n",
        "train_sentences = [[word if word in train_vocab else \"<UNK>\" for word in sentence] for sentence in train_sentences_noUNK]\n",
        "train_sentences_unpad = [[word if word in train_vocab else \"<UNK>\" for word in sentence] for sentence in train_sentences_unpad_noUNK]\n",
        "\n",
        "dev_sentences, dev_sentences_unpad, dev_vocab = preprocess_file_nltk(dev_file)\n",
        "test_sentences, test_sentences_unpad, test_vocab = preprocess_file_nltk(test_file)\n",
        "print(train_sentences[:10])\n",
        "\n",
        "print(f\"Train vocabulary size: {len(train_vocab)}\")\n",
        "print(f\"Dev vocabulary size: {len(dev_vocab)}\")\n",
        "print(f\"Test vocabulary size: {len(test_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQNsUA5kfZe"
      },
      "source": [
        "**Discussion**\n",
        "Given the vocabulary size above, the number of parameters in a n-gram model is simply vocab size to its nth times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzDNMVikkeX"
      },
      "source": [
        "### 2.2 $N$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxkcs2HykuR2"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=3 unk_label='<UNK>' and 17658 items>\n",
            "<Vocabulary with cutoff=3 unk_label='<UNK>' and 17658 items>\n",
            "Unigram Model Perplexity: 835.112106023236\n",
            "Bigram Model Perplexity: inf\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from nltk import everygrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten\n",
        "\n",
        "def train_language_model(n, train_data, vocab):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    # Train the language model\n",
        "    lm = MLE(n, vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    print(lm.vocab)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "def compute_perplexity(n, lm, test_data):\n",
        "    # Preprocess the test data\n",
        "    padded_sentences = [list(pad_both_ends(sent, n)) for sent in test_data]\n",
        "    # flattened_sentences = list(flatten(test_data))\n",
        "    # test_data, padded_sents = padded_everygram_pipeline(n, test_data)\n",
        "    test_ngrams = [ngram for sent in test_data for ngram in everygrams(sent, max_len=n)]\n",
        "    # Calculate perplexity\n",
        "    return lm.perplexity(test_ngrams)\n",
        "\n",
        "# Train models and calculate perplexity\n",
        "unigram_lm = train_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_language_model(2, train_sentences, train_vocab)\n",
        "\n",
        "unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "print(\"Unigram Model Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Model Perplexity:\", bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3o9Nez8kvYm"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "The infinity from Bigram model comes from a 0 numerator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQUqM73kzf-"
      },
      "source": [
        "### 2.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LgXRmJwk3Y-"
      },
      "source": [
        "#### 2.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG7jCIRk7Qw"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Perplexity: 836.0150916297277\n",
            "Bigram Perplexity: 1006.1865977200891\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm import Laplace\n",
        "\n",
        "def train_laplace_language_model(n, train_data, vocab):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    # Train the language model with Laplace smoothing\n",
        "    lm = Laplace(n, vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "# Train the models with Laplace smoothing\n",
        "unigram_lm = train_laplace_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_laplace_language_model(2, train_sentences, train_vocab)\n",
        "\n",
        "# Calculate perplexity\n",
        "unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "print(\"Unigram Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Perplexity:\", bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yTKPXFk8f2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cFbczqlBR_"
      },
      "source": [
        "#### 2.3.2: Add-$k$ smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_ZiAgIlPUu"
      },
      "source": [
        "**Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Perplexity: 835.1135795103045\n",
            "Bigram Perplexity: 495.6881461545059\n"
          ]
        }
      ],
      "source": [
        "from nltk.lm import Lidstone\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def train_lidstone_language_model(n, train_data, vocab, lmda):\n",
        "    # Create an n-gram generator with padding\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    \n",
        "    # Train the language model with Lidstone smoothing\n",
        "    lm = Lidstone(lmda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    \n",
        "    return lm\n",
        "\n",
        "# Train the models with Lidstone smoothing (choose a value for lambda, e.g., 0.5)\n",
        "lmda = 5e-3\n",
        "# Train the models with Laplace smoothing\n",
        "unigram_lm = train_lidstone_language_model(1, train_sentences, train_vocab, lmda)\n",
        "bigram_lm = train_lidstone_language_model(2, train_sentences, train_vocab, lmda)\n",
        "\n",
        "# Calculate perplexity\n",
        "unigram_perplexity = compute_perplexity(1, unigram_lm, dev_sentences)\n",
        "bigram_perplexity = compute_perplexity(2, bigram_lm, dev_sentences)\n",
        "\n",
        "print(\"Unigram Perplexity:\", unigram_perplexity)\n",
        "print(\"Bigram Perplexity:\", bigram_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFNf8OIlQ0O"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKEO_TqlUrX"
      },
      "source": [
        "#### 2.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdd4cvYlZuO"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Train MLE models with different n-grams\n",
        "\n",
        "\n",
        "# def interpolated_probability(ngram, lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm):\n",
        "#     unigram_prob = unigram_lm.score(ngram[-1])\n",
        "#     bigram_prob = bigram_lm.score(ngram[-2:], ngram[:-1])\n",
        "#     trigram_prob = trigram_lm.score(ngram, ngram[:-1])\n",
        "\n",
        "#     return lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
        "\n",
        "# def interpolated_perplexity(lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm, dataset):\n",
        "#     log_prob_sum = 0\n",
        "#     token_count = 0\n",
        "#     epsilon = 1e-10\n",
        "\n",
        "#     for sentence in dataset:\n",
        "#         for i in range(2, len(sentence)):\n",
        "#             ngram = tuple(sentence[i-2:i+1])\n",
        "#             prob = interpolated_probability(ngram, lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm)\n",
        "#             log_prob_sum += math.log(prob + epsilon) \n",
        "#             token_count += 1\n",
        "\n",
        "#     return math.exp(-log_prob_sum / token_count)\n",
        "\n",
        "# Train unigram, bigram, and trigram models\n",
        "\n",
        "\n",
        "# models = [unigram_lm, bigram_lm, trigram_lm]\n",
        "\n",
        "# # in interpolation, we always mix results of all models with trained lambda values from dev set\n",
        "# # Optimize hyperparameters (lambdas) on the dev set\n",
        "# best_lambdas = [0.0, 0.0, 0.0]\n",
        "# print(np.arange(0, 1.1, 0.1))\n",
        "# best_dev_perplexity = float('inf')\n",
        "# for lambda1 in np.arange(0, 1.1, 0.1):\n",
        "#     for lambda2 in np.arange(0, 1.1 - lambda1, 0.1):\n",
        "#         lambda3 = 1 - lambda1 - lambda2\n",
        "\n",
        "#         perplexity = interpolated_perplexity(lambda1, lambda2, lambda3, unigram_lm, bigram_lm, trigram_lm, dev_sentences)\n",
        "#         if perplexity < best_dev_perplexity:\n",
        "#             best_dev_perplexity = perplexity\n",
        "#             best_lambdas[0], best_lambdas[1], best_lambdas[2] = lambda1, lambda2, lambda3\n",
        "#             print(\"Best lambda1:\", lambda1, \"Best lambda2:\", lambda2, \"Best lambda3:\", lambda3, \"Best perplexity:\", perplexity)\n",
        "\n",
        "# print(\"Finished training hyperparameters\")\n",
        "\n",
        "# # Report perplexity on the training and dev sets\n",
        "# # train_perplexity = compute_interpolated_perplexity(models, best_lambdas, train_sentences)\n",
        "# train_perplexity = interpolated_perplexity(best_lambdas[0], best_lambdas[1], best_lambdas[2], unigram_lm, bigram_lm, trigram_lm, train_sentences)\n",
        "# print(\"Best Lambdas:\", best_lambdas)\n",
        "# print(\"Training Perplexity:\", train_perplexity)\n",
        "# print(\"Dev Perplexity:\", best_dev_perplexity)\n",
        "\n",
        "# # Report perplexity on the test set\n",
        "# test_perplexity = interpolated_perplexity(best_lambdas[0], best_lambdas[1], best_lambdas[2], unigram_lm, bigram_lm, trigram_lm, test_sentences)\n",
        "# print(\"Test Perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def train_mle_language_model(n, train_data, vocab, lidstone_lambda=5e-3):\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n, train_data)\n",
        "    # lm = MLE(n, vocabulary=vocab)\n",
        "    lm = Lidstone(lidstone_lambda, order=n,vocabulary=vocab)\n",
        "    lm.fit(train_data, padded_sents)\n",
        "    return lm\n",
        "\n",
        "unigram_lm = train_mle_language_model(1, train_sentences, train_vocab)\n",
        "bigram_lm = train_mle_language_model(2, train_sentences, train_vocab)\n",
        "trigram_lm = train_mle_language_model(3, train_sentences, train_vocab)\n",
        "print(\"Finished training language models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/k2/7hr6hcfd5v58p9nhhf_brbbm0000gn/T/ipykernel_4039/39630103.py:16: RuntimeWarning: invalid value encountered in log\n",
            "  perplexity += np.log(sentence_perplexity)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best coefficients: [0.2, 0.7000000000000001, 0.09999999999999998]\n",
            "Minimum perplexity: 194.88369622296648\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def perplexity_of_interpolated_model(lambdas, unigram_lm, bigram_lm, trigram_lm, dev_sentences):\n",
        "    perplexity = 0\n",
        "    total_words = 0\n",
        "    \n",
        "    for sentence in dev_sentences:\n",
        "        sentence_perplexity = 1\n",
        "        for i in range(len(sentence)):\n",
        "            unigram_prob = unigram_lm.score(sentence[i])\n",
        "            bigram_prob = bigram_lm.score(sentence[i], sentence[max(i - 1, 0):i])\n",
        "            trigram_prob = trigram_lm.score(sentence[i], sentence[max(i - 2, 0):i])\n",
        "\n",
        "            interpolated_prob = lambdas[0] * unigram_prob + lambdas[1] * bigram_prob + lambdas[2] * trigram_prob\n",
        "            sentence_perplexity *= (1 / interpolated_prob)\n",
        "        \n",
        "        total_words += len(sentence)\n",
        "        perplexity += np.log(sentence_perplexity)\n",
        "    \n",
        "    return np.exp(perplexity / total_words)\n",
        "\n",
        "# Find the coefficients that minimize the perplexity\n",
        "best_lambdas = None\n",
        "best_perplexity = float('inf')\n",
        "\n",
        "# print(np.arange(0, 1.1, 0.1))\n",
        "for lambda1 in np.arange(0, 1.1, 0.1):\n",
        "    for lambda2 in np.arange(0, 1.1 - lambda1, 0.1):\n",
        "        lambda3 = 1 - lambda1 - lambda2\n",
        "        lambdas = [lambda1, lambda2, lambda3]\n",
        "\n",
        "        current_perplexity = perplexity_of_interpolated_model(lambdas, unigram_lm, bigram_lm, trigram_lm, dev_sentences)\n",
        "        \n",
        "        if current_perplexity < best_perplexity:\n",
        "            best_perplexity = current_perplexity\n",
        "            best_lambdas = lambdas\n",
        "\n",
        "print(\"Best coefficients:\", best_lambdas)\n",
        "print(\"Minimum perplexity:\", best_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKqmQ37lcH2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzSbk2bClf3u"
      },
      "source": [
        "##### **Optimization**:\n",
        "\n",
        "\\# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTcTlLuloHu"
      },
      "source": [
        "## 3 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jb0OQ-yltc3"
      },
      "outputs": [],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://github.com/qtli/COMP7607-Fall2023/blob/master/assignments/A1/data/prep/dev.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 RoBERTa Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_input_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    text = text.replace('<PREP>', '<mask>')\n",
        "    with open(\"./data/prep/dev_mask.in\", 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "input_file = \"./data/prep/dev.in\"\n",
        "preprocessed_file = \"./data/prep/dev_mask.in\"\n",
        "preprocess_input_file(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/home/public/anaconda3/envs/Junheng_torch/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 03:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer, LineByLineTextDataset, DataCollatorForLanguageModeling, RobertaForMaskedLM, Trainer, TrainingArguments, pipeline\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=preprocessed_file,\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=1.0\n",
        ")\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./fine_tuned_roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['on', 'of', 'in', 'in']\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = RobertaForMaskedLM.from_pretrained(\"../A1/fine_tuned_roberta\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Create a fill-mask pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=fine_tuned_model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Predict masked words\n",
        "sentence = \"palestinian leader yasser arafat <mask> wednesday welcomed the resumption <mask> israeli-syrian peace talks , which were due to begin later <mask> the day <mask> the united states.\"\n",
        "masked_sentence = sentence.replace('<PREP>', '<mask>')\n",
        "predictions = fill_mask(masked_sentence)\n",
        "# print(predictions)\n",
        "\n",
        "# Filter and print preposition predictions\n",
        "prepositions = [\"at\", \"in\", \"on\", \"for\", \"of\"]\n",
        "\n",
        "preposition_predictions = []\n",
        "for mask_preds in predictions:\n",
        "    for pred in mask_preds:\n",
        "        token = pred['token_str'].strip()\n",
        "        if token in prepositions:\n",
        "            preposition_predictions.append(token)\n",
        "            break\n",
        "\n",
        "print(preposition_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correction rate: 76.90%\n"
          ]
        }
      ],
      "source": [
        "with open(\"./data/prep/test.in\", \"r\", encoding=\"utf-8\") as f:\n",
        "    input = f.readlines()\n",
        "    test_sentences = []\n",
        "    for line in input:\n",
        "        test_sentences.append(line.replace('<PREP>', '<mask>'))\n",
        "        \n",
        "with open(\"./data/prep/test.out\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_answer = f.readlines()\n",
        "\n",
        "# print(test_sentences)\n",
        "# print(test_out_lines)\n",
        "\n",
        "def get_predictions(sentence):\n",
        "    preposition_predictions = []\n",
        "    predictions = fill_mask(sentence)\n",
        "    for pred in predictions:\n",
        "        if type(pred) is dict:\n",
        "            token = pred['token_str'].strip()\n",
        "            if token in prepositions:\n",
        "                preposition_predictions.append(token)\n",
        "                break\n",
        "        else:\n",
        "            for candidate in pred:\n",
        "                token = candidate['token_str'].strip()\n",
        "                if token in prepositions:\n",
        "                    preposition_predictions.append(token)\n",
        "                    break\n",
        "    return preposition_predictions\n",
        "\n",
        "# Evaluate the model's performance\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for test_in_line, test_out_line in zip(test_sentences, test_answer):\n",
        "    predictions = get_predictions(test_in_line)\n",
        "    correct_answers = test_out_line.strip().split()\n",
        "    total_count += len(correct_answers)\n",
        "\n",
        "    for pred, correct in zip(predictions, correct_answers):\n",
        "        if pred == correct:\n",
        "            correct_count += 1\n",
        "\n",
        "# Calculate and print the correction rate\n",
        "correction_rate = correct_count / total_count\n",
        "print(f\"Correction rate: {correction_rate:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 LSTM Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create validation files according to correct answer\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "    return content\n",
        "\n",
        "def write_file(file_path, content):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def fill_prep_words(input_file, output_file, result_file):\n",
        "    input_content = read_file(input_file)\n",
        "    output_content = read_file(output_file)\n",
        "\n",
        "    input_sentences = input_content.strip().split('\\n')\n",
        "    output_preps = output_content.strip().split()\n",
        "\n",
        "    output_index = 0\n",
        "    result_sentences = []\n",
        "\n",
        "    for sentence in input_sentences:\n",
        "        words = sentence.split()\n",
        "        filled_words = []\n",
        "\n",
        "        for word in words:\n",
        "            if word == '<PREP>':\n",
        "                filled_words.append(output_preps[output_index])\n",
        "                output_index += 1\n",
        "            else:\n",
        "                filled_words.append(word)\n",
        "\n",
        "        result_sentences.append(' '.join(filled_words))\n",
        "\n",
        "    result_content = '\\n'.join(result_sentences)\n",
        "    write_file(result_file, result_content)\n",
        "\n",
        "# Main code execution\n",
        "input_file = './data/prep/dev.in'\n",
        "output_file = './data/prep/dev.out'\n",
        "result_file = './data/prep/validate.in'\n",
        "fill_prep_words(input_file, output_file, result_file)\n",
        "\n",
        "input_file = './data/prep/test.in'\n",
        "output_file = './data/prep/test.out'\n",
        "result_file = './data/prep/test_validate.in'\n",
        "fill_prep_words(input_file, output_file, result_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Replace these with the paths to your training and validation files\n",
        "train_data_path = './data/prep/dev.in'\n",
        "val_data_path = './data/prep/validate.in'\n",
        "\n",
        "with open(train_data_path) as f:\n",
        "    train_data = f.read().splitlines()\n",
        "\n",
        "with open(val_data_path) as f:\n",
        "    val_data = f.read().splitlines()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(filters='', lower=False)\n",
        "tokenizer.fit_on_texts(train_data + val_data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Generate sequences and labels\n",
        "def generate_sequences(data):\n",
        "    input_sequences, labels = [], []\n",
        "    for line in data:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence[:-1])\n",
        "            labels.append(n_gram_sequence[-1])\n",
        "    return input_sequences, labels\n",
        "\n",
        "train_sequences, train_labels = generate_sequences(train_data)\n",
        "val_sequences, val_labels = generate_sequences(val_data)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(seq) for seq in train_sequences + val_sequences])\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "val_sequences = pad_sequences(val_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# One-hot encode labels\n",
        "train_labels = to_categorical(train_labels, num_classes=total_words)\n",
        "val_labels = to_categorical(val_labels, num_classes=total_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_sequences, train_labels, epochs=100, validation_data=(val_sequences, val_labels))\n",
        "model.save('lstm_masked_word_prediction.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
